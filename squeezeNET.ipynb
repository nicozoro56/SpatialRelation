{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "!pip install gdown\n",
    "import gdown  # Install gdown on Kaggle using pip if necessary\n",
    "\n",
    "# --- Set Up Directories ---\n",
    "tmp_dir = Path(\"/kaggle/temp\")\n",
    "target_dir = Path(\"/kaggle/working/spatialsense_data\")\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Download and Extract SpatialSense Dataset ---\n",
    "print(\"Downloading and extracting SpatialSense dataset...\")\n",
    "spatialsense_dir = target_dir / \"spatialsense\"\n",
    "spatialsense_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spatialsense_image_dir = spatialsense_dir / \"images\"\n",
    "spatialsense_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Download and unzip `spatialsense.zip`\n",
    "spatialsense_zip_url = \"https://zenodo.org/api/records/8104370/files-archive\"\n",
    "spatialsense_zip_file = tmp_dir / \"spatialsense.zip\"\n",
    "subprocess.run([\"wget\", spatialsense_zip_url, \"-O\", str(spatialsense_zip_file)], check=True)\n",
    "\n",
    "# Unzip SpatialSense archive\n",
    "subprocess.run([\"unzip\", \"-o\", str(spatialsense_zip_file), \"-d\", str(spatialsense_dir)], check=True)\n",
    "\n",
    "# Step 2: Extract `images.tar.gz`\n",
    "spatialsense_images_tar = spatialsense_dir / \"images.tar.gz\"\n",
    "subprocess.run([\"tar\", \"-zxvf\", str(spatialsense_images_tar), \"-C\", str(spatialsense_image_dir)], check=True)\n",
    "\n",
    "# --- Download SpatialSense+ Annotations ---\n",
    "print(\"Downloading SpatialSense+ annotations...\")\n",
    "gdrive_link = \"https://docs.google.com/uc?export=download&id=1vIOozqk3OlxkxZgL356pD1EAGt06ZwM4\"\n",
    "annotations_file = spatialsense_dir / \"annots_spatialsenseplus.json\"\n",
    "gdown.download(url=gdrive_link, output=str(annotations_file), quiet=False)\n",
    "\n",
    "# --- Cleanup Temporary Files ---\n",
    "print(\"Cleaning up temporary files...\")\n",
    "spatialsense_zip_file.unlink(missing_ok=True)  # Remove the zip file\n",
    "tmp_dir.rmdir()  # Remove the temp directory\n",
    "\n",
    "print(\"Download and extraction completed.\")\n",
    "print(f\"Dataset extracted to: {spatialsense_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class SpatialSenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, predicates, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.predicates = predicates\n",
    "        self.predicate_to_index = {pred: idx for idx, pred in enumerate(predicates)}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        image_path = annotation[\"image_path\"]\n",
    "        predicate = annotation[\"predicate\"]\n",
    "        subject_bbox = annotation[\"subject_bbox\"]\n",
    "        object_bbox = annotation[\"object_bbox\"]\n",
    "        subject_name = annotation[\"subject_name\"]\n",
    "        object_name = annotation[\"object_name\"]\n",
    "\n",
    "        # Load and crop the image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            cropped_image = image.crop((\n",
    "                min(subject_bbox[2], object_bbox[2]),  # x0\n",
    "                min(subject_bbox[0], object_bbox[0]),  # y0\n",
    "                max(subject_bbox[3], object_bbox[3]),  # x1\n",
    "                max(subject_bbox[1], object_bbox[1])   # y1\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Image {image_path} could not be loaded: {str(e)}\")\n",
    "\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "        label = torch.tensor(self.predicate_to_index[predicate], dtype=torch.long)\n",
    "        return cropped_image, label, subject_name, object_name\n",
    "\n",
    "# --- Parse Annotations ---\n",
    "def parse_annotations(annotations_file, extracted_images_path):\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    for ann in annotations.get('sample_annots', []):\n",
    "        url = ann.get('url')\n",
    "        split = ann.get('split')\n",
    "\n",
    "        for pred_ann in ann.get('annotations', []):\n",
    "            predicate = pred_ann.get(\"predicate\")\n",
    "            label = pred_ann.get(\"label\")\n",
    "            subject = pred_ann.get(\"subject\", {})\n",
    "            object_ = pred_ann.get(\"object\", {})\n",
    "\n",
    "            if not url or not split or predicate not in predicates or str(label) != \"True\":\n",
    "                continue\n",
    "\n",
    "            folder = \"flickr\" if \"staticflickr\" in url else \"nyu\" if \"nyu\" in url else None\n",
    "            if folder is None:\n",
    "                continue\n",
    "\n",
    "            filename = os.path.basename(url)\n",
    "            if filename.startswith(\"._\"):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(extracted_images_path, folder, filename)\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            dataset.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"predicate\": predicate,\n",
    "                \"split\": split,\n",
    "                \"subject_bbox\": subject.get(\"bbox\", [0, 0, 0, 0]),\n",
    "                \"object_bbox\": object_.get(\"bbox\", [0, 0, 0, 0]),\n",
    "                \"subject_name\": subject.get(\"name\", \"unknown\"),\n",
    "                \"object_name\": object_.get(\"name\", \"unknown\"),\n",
    "            })\n",
    "    return dataset\n",
    "\n",
    "# --- Early Stopping ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "\n",
    "# --- Data Preparation ---\n",
    "annotations_path = \"/kaggle/working/spatialsense_data/spatialsense/annots_spatialsenseplus.json\"\n",
    "extracted_images_path = \"/kaggle/working/spatialsense_data/spatialsense/images/images\"\n",
    "#predicates = [\"above\", \"behind\", \"in\", \"in front of\", \"next to\", \"on\", \"to the left of\", \"to the right of\", \"under\"]\n",
    "predicates = [\"above\", \"to the left of\", \"to the right of\", \"under\"]\n",
    "data = parse_annotations(annotations_path, extracted_images_path)\n",
    "train_data = [item for item in data if item['split'] == 'train']\n",
    "valid_data = [item for item in data if item['split'] == 'valid']\n",
    "\n",
    "# --- Transforms ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = SpatialSenseDataset(train_data, predicates, transform=transform_train)\n",
    "valid_dataset = SpatialSenseDataset(valid_data, predicates, transform=transform_valid)\n",
    "\n",
    "# --- Count Images Per Predicate ---\n",
    "predicate_counts = Counter([item[\"predicate\"] for item in train_data])\n",
    "total_images = sum(predicate_counts.values())\n",
    "\n",
    "# --- Class Weights ---\n",
    "class_weights = [\n",
    "    total_images / predicate_counts[pred] if predicate_counts[pred] > 0 else 0\n",
    "    for pred in predicates\n",
    "]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# --- Data Loaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# --- Feature Extractor ---\n",
    "class SqueezeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SqueezeNetFeatureExtractor, self).__init__()\n",
    "        squeezenet = squeezenet1_1(weights=SqueezeNet1_1_Weights.IMAGENET1K_V1) #same as default\n",
    "        for param in squeezenet.features.parameters():\n",
    "            param.requires_grad = True  \n",
    "        self.features = squeezenet.features\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "feature_extractor = SqueezeNetFeatureExtractor().to(device)\n",
    "\n",
    "# --- MLP Model with Increased Dropout ---\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 216),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(216),\n",
    "            nn.Linear(216, num_classes),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        #self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- Determine Input Dim ---\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    extracted_features = feature_extractor(dummy_input)\n",
    "input_dim = extracted_features.view(-1).shape[0]\n",
    "\n",
    "mlp_model = MLPModel(input_dim=input_dim, num_classes=len(predicates)).to(device)\n",
    "\n",
    "# --- Weighted Loss Function ---\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor,label_smoothing=0.0)\n",
    "\n",
    "# --- Weighted Loss and Weight Decay ---\n",
    "optimizer = optim.SGD([\n",
    "    {\"params\": feature_extractor.parameters(), \"lr\": 1e-3, \"momentum\": 0.9},\n",
    "    {\"params\": mlp_model.parameters(), \"lr\": 1e-3, \"momentum\": 0.9}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1)\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, feature_extractor, mlp_model):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mlp_model = mlp_model\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.forward_features(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten the features\n",
    "        return self.mlp_model(features)\n",
    "\n",
    "full_model = FullModel(feature_extractor, mlp_model).to(device)\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "def train_one_epoch(loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (images, labels, subject_names, object_names) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Metrics calculation\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# --- Update Evaluation Function ---\n",
    "def evaluate(loader, model, criterion, detailed_metrics=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, subject_names, object_names) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Metrics\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    if detailed_metrics:\n",
    "        precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=range(len(predicates)))\n",
    "        return epoch_loss, epoch_accuracy, precision, recall, f1, cm\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"valid_precision\": [],\n",
    "    \"valid_recall\": [],\n",
    "    \"valid_f1\": []\n",
    "}\n",
    "# --- Main Functionality ---\n",
    "def train_and_evaluate(\n",
    "    train_loader, valid_loader, model, criterion, optimizer, scheduler, \n",
    "    target_layer, predicates, num_epochs=20, patience=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model, with options for early stopping, Grad-CAM visualization, and node ablation.\n",
    "\n",
    "    Parameters:\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        valid_loader (DataLoader): DataLoader for validation data.\n",
    "        model (nn.Module): Full model (feature extractor + MLP).\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        target_layer (nn.Module): Target layer for Grad-CAM visualization.\n",
    "        predicates (list): List of predicates (class labels).\n",
    "        num_epochs (int): Maximum number of epochs.\n",
    "        patience (int): Patience for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_acc = train_one_epoch(train_loader, model, optimizer, criterion)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        valid_loss, valid_acc, valid_prec, valid_rec, valid_f1, cm = evaluate(valid_loader, model, criterion, detailed_metrics=True)\n",
    "        print(f\"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_acc:.4f}\")\n",
    "        print(f\"Precision: {valid_prec:.4f}, Recall: {valid_rec:.4f}, F1: {valid_f1:.4f}\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"valid_precision\"].append(valid_prec)\n",
    "        history[\"valid_recall\"].append(valid_rec)\n",
    "        history[\"valid_f1\"].append(valid_f1)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Restoring best model.\")\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "\n",
    "    # Grad-CAM and Node Ablation\n",
    "    print(\"Generating Grad-CAM heatmaps and performing node ablation...\")\n",
    "    generate_heatmaps_and_ablate(\n",
    "        valid_loader, model, target_layer, criterion, predicates, device, n_samples=10\n",
    "    )\n",
    "\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=predicates)\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "    plt.show()\n",
    "# --- Heatmap Generation and Node Ablation ---\n",
    "def generate_heatmaps_and_ablate(loader, model, target_layer, criterion, predicates, device, n_samples=10):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM heatmaps and perform node ablation analysis for a subset of images.\n",
    "\n",
    "    Parameters:\n",
    "        loader (DataLoader): DataLoader for the dataset.\n",
    "        model (nn.Module): Full model (feature extractor + MLP).\n",
    "        target_layer (nn.Module): Target layer for Grad-CAM visualization.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        predicates (list): List of predicates (class labels).\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        n_samples (int): Number of samples to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from pytorch_grad_cam import GradCAM\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    grad_cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "    model.eval()\n",
    "\n",
    "    processed = 0\n",
    "    for batch_idx, (images, labels, subject_names, object_names) in enumerate(loader):\n",
    "        if processed >= n_samples:\n",
    "            break\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)  \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            if processed >= n_samples:\n",
    "                break\n",
    "\n",
    "            image = images[i].unsqueeze(0)  \n",
    "            label = labels[i].item()\n",
    "            pred = preds[i].item()\n",
    "        \n",
    "            subject_name = subject_names[i]\n",
    "            object_name = object_names[i]\n",
    "\n",
    "            # Generate heatmap\n",
    "            grayscale_cam = grad_cam(input_tensor=image)[0]\n",
    "            original_image = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            original_image = (original_image * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # Unnormalize\n",
    "            original_image = np.clip(original_image, 0, 1)\n",
    "            heatmap = show_cam_on_image(original_image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "            # Display the heatmap\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(original_image)\n",
    "            plt.title(\n",
    "                f\"Original Image\\n\"\n",
    "                f\"Subject: {subject_name}, Object: {object_name}\\n\"\n",
    "                f\"True: {predicates[label]}, Predicted: {predicates[pred]}\"\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(heatmap)\n",
    "            plt.title(\"Grad-CAM Heatmap\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "        # Node Ablation Analysis\n",
    "        features = model.forward_features(images)\n",
    "        for i in range(features.size(1)):  # Iterate over feature dimensions\n",
    "            modified_features = features.clone()\n",
    "            modified_features[:, i] = 0  # Zero out one feature dimension\n",
    "\n",
    "            outputs_modified = model.mlp_model(modified_features)\n",
    "            loss_modified = criterion(outputs_modified, labels)\n",
    "\n",
    "\n",
    "    print(\"Heatmap generation and node ablation completed.\")\n",
    "\n",
    "# --- Call the Training and Evaluation Function ---\n",
    "target_layer = feature_extractor.features[-1] \n",
    "train_and_evaluate(\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    model=full_model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    target_layer=target_layer,\n",
    "    predicates=predicates,\n",
    "    num_epochs=50,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"valid_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs. Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"valid_precision\"], label=\"Precision\")\n",
    "    plt.plot(history[\"valid_recall\"], label=\"Recall\")\n",
    "    plt.plot(history[\"valid_f1\"], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Validation Metrics: Precision, Recall, F1-Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- Display Plots ---\n",
    "plot_loss(history)\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
