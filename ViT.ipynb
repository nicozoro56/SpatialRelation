{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "!pip install gdown\n",
    "import gdown  # Install gdown on Kaggle using pip if necessary\n",
    "\n",
    "# --- Set Up Directories ---\n",
    "tmp_dir = Path(\"/kaggle/temp\")\n",
    "target_dir = Path(\"/kaggle/working/spatialsense_data\")\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Download and Extract SpatialSense Dataset ---\n",
    "print(\"Downloading and extracting SpatialSense dataset...\")\n",
    "spatialsense_dir = target_dir / \"spatialsense\"\n",
    "spatialsense_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spatialsense_image_dir = spatialsense_dir / \"images\"\n",
    "spatialsense_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Step 1: Download and unzip `spatialsense.zip`\n",
    "spatialsense_zip_url = \"https://zenodo.org/api/records/8104370/files-archive\"\n",
    "spatialsense_zip_file = tmp_dir / \"spatialsense.zip\"\n",
    "subprocess.run([\"wget\", spatialsense_zip_url, \"-O\", str(spatialsense_zip_file)], check=True)\n",
    "\n",
    "# Unzip SpatialSense archive\n",
    "subprocess.run([\"unzip\", \"-o\", str(spatialsense_zip_file), \"-d\", str(spatialsense_dir)], check=True)\n",
    "\n",
    "# Step 2: Extract `images.tar.gz`\n",
    "spatialsense_images_tar = spatialsense_dir / \"images.tar.gz\"\n",
    "subprocess.run([\"tar\", \"-zxvf\", str(spatialsense_images_tar), \"-C\", str(spatialsense_image_dir)], check=True)\n",
    "\n",
    "# --- Download SpatialSense+ Annotations ---\n",
    "print(\"Downloading SpatialSense+ annotations...\")\n",
    "gdrive_link = \"https://docs.google.com/uc?export=download&id=1vIOozqk3OlxkxZgL356pD1EAGt06ZwM4\"\n",
    "annotations_file = spatialsense_dir / \"annots_spatialsenseplus.json\"\n",
    "gdown.download(url=gdrive_link, output=str(annotations_file), quiet=False)\n",
    "\n",
    "# --- Cleanup Temporary Files ---\n",
    "print(\"Cleaning up temporary files...\")\n",
    "spatialsense_zip_file.unlink(missing_ok=True)  # Remove the zip file\n",
    "tmp_dir.rmdir()  # Remove the temp directory\n",
    "\n",
    "print(\"Download and extraction completed.\")\n",
    "print(f\"Dataset extracted to: {spatialsense_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "import pytorch_grad_cam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class SpatialSenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, predicates, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.predicates = predicates\n",
    "        self.predicate_to_index = {pred: idx for idx, pred in enumerate(predicates)}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        image_path = annotation[\"image_path\"]\n",
    "        predicate = annotation[\"predicate\"]\n",
    "        subject_bbox = annotation[\"subject_bbox\"]\n",
    "        object_bbox = annotation[\"object_bbox\"]\n",
    "        subject_name = annotation[\"subject_name\"]\n",
    "        object_name = annotation[\"object_name\"]\n",
    "\n",
    "        # Load and crop the image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            cropped_image = image.crop((\n",
    "                min(subject_bbox[2], object_bbox[2]),  # x0\n",
    "                min(subject_bbox[0], object_bbox[0]),  # y0\n",
    "                max(subject_bbox[3], object_bbox[3]),  # x1\n",
    "                max(subject_bbox[1], object_bbox[1])   # y1\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Image {image_path} could not be loaded: {str(e)}\")\n",
    "\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "        label = torch.tensor(self.predicate_to_index[predicate], dtype=torch.long)\n",
    "        return cropped_image, label, subject_name, object_name\n",
    "\n",
    "def parse_annotations(annotations_file, extracted_images_path):\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    for ann in annotations.get('sample_annots', []):\n",
    "        url = ann.get('url')\n",
    "        split = ann.get('split')\n",
    "\n",
    "        for pred_ann in ann.get('annotations', []):\n",
    "            predicate = pred_ann.get(\"predicate\")\n",
    "            label = pred_ann.get(\"label\")\n",
    "            subject = pred_ann.get(\"subject\", {})\n",
    "            object_ = pred_ann.get(\"object\", {})\n",
    "\n",
    "            if not url or not split or predicate not in predicates or str(label) != \"True\":\n",
    "                continue\n",
    "\n",
    "            folder = \"flickr\" if \"staticflickr\" in url else \"nyu\" if \"nyu\" in url else None\n",
    "            if folder is None:\n",
    "                continue\n",
    "\n",
    "            filename = os.path.basename(url)\n",
    "            if filename.startswith(\"._\"):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(extracted_images_path, folder, filename)\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            dataset.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"predicate\": predicate,\n",
    "                \"split\": split,\n",
    "                \"subject_bbox\": subject.get(\"bbox\", [0, 0, 0, 0]),\n",
    "                \"object_bbox\": object_.get(\"bbox\", [0, 0, 0, 0]),\n",
    "                \"subject_name\": subject.get(\"name\", \"unknown\"),\n",
    "                \"object_name\": object_.get(\"name\", \"unknown\"),\n",
    "            })\n",
    "    return dataset\n",
    "\n",
    "# --- Vision Transformer Feature Extractor ---\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTFeatureExtractor, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values=x)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor().to(device)\n",
    "\n",
    "# --- MLP Model with Increased Dropout ---\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 216),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(216),\n",
    "            nn.Linear(216, num_classes),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# --- Full Model ---\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, feature_extractor, mlp_model):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mlp_model = mlp_model\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.forward_features(x)\n",
    "        return self.mlp_model(features)\n",
    "\n",
    "# --- Training and Evaluation Function ---\n",
    "def train_and_evaluate(\n",
    "    train_loader, valid_loader, model, criterion, optimizer, scheduler, target_layer, \n",
    "    predicates, device, num_epochs=100, patience=5\n",
    "):\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"valid_precision\": [],\n",
    "        \"valid_recall\": [],\n",
    "        \"valid_f1\": []\n",
    "    }\n",
    "\n",
    "    best_f1 = 0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_predictions_train = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for images, labels, _, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions_train += (preds == labels).sum().item()\n",
    "            total_samples_train += labels.size(0)\n",
    "\n",
    "        train_loss = total_train_loss / total_samples_train\n",
    "        train_accuracy = correct_predictions_train / total_samples_train\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        correct_predictions_valid = 0\n",
    "        total_samples_valid = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _, _ in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_valid_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_predictions_valid += (preds == labels).sum().item()\n",
    "                total_samples_valid += labels.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        valid_loss = total_valid_loss / total_samples_valid\n",
    "        valid_accuracy = correct_predictions_valid / total_samples_valid\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"valid_precision\"].append(precision)\n",
    "        history[\"valid_recall\"].append(recall)\n",
    "        history[\"valid_f1\"].append(f1)\n",
    "\n",
    "        print(f\"Valid Loss: {valid_loss:.4f} - Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "        print(f\"Valid Precision: {precision:.4f} - Valid Recall: {recall:.4f} - Valid F1: {f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    return history\n",
    "\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "\n",
    "# Updated reshape_transform for ViTs\n",
    "def reshape_transform(activations):\n",
    "    if isinstance(activations, tuple):\n",
    "        activations = activations[0]  # Extract tensor from tuple\n",
    "    \n",
    "    activations = activations[:, 1:, :]  # Exclude CLS token\n",
    "    h, w = int(activations.shape[1] ** 0.5), int(activations.shape[1] ** 0.5)\n",
    "    activations = activations.transpose(1, 2).reshape(activations.shape[0], -1, h, w)\n",
    "    return activations\n",
    "\n",
    "def generate_eigencam_vit(loader, model, dataset, n_samples=None):\n",
    "    \"\"\"\n",
    "    Visualizes EigenCAM along with object name, subject name, and predicate information for the given number of samples.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader containing the dataset.\n",
    "        model: The ViT-based model for prediction.\n",
    "        dataset: The dataset instance to access metadata (e.g., subject, predicate, object).\n",
    "        n_samples: Number of samples to visualize. If None, all samples will be processed.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    target_layer = model.feature_extractor.vit.encoder.layer[-1]  # Use the last transformer block\n",
    "\n",
    "    # Initialize EigenCAM\n",
    "    cam = EigenCAM(\n",
    "        model=model,\n",
    "        target_layers=[target_layer],\n",
    "        reshape_transform=reshape_transform,\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    for batch_idx, (images, labels, subject_names, object_names) in enumerate(loader):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Define the target classes\n",
    "        targets = [ClassifierOutputTarget(label.item()) for label in labels]\n",
    "\n",
    "        # Generate CAM for the batch\n",
    "        grayscale_cams = cam(input_tensor=images, targets=targets)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)  # Forward pass\n",
    "            preds = torch.argmax(outputs, dim=1)  # Predicted class\n",
    "\n",
    "        # Visualize and display metadata for each sample, but limit to n_samples\n",
    "        for i in range(images.size(0)):\n",
    "            if n_samples is not None and count >= n_samples:\n",
    "                break\n",
    "\n",
    "            # Extract metadata for the current sample\n",
    "            subject = subject_names[i]\n",
    "            object_name = object_names[i]\n",
    "            ground_truth = dataset.predicates[labels[i].item()]  # Ground truth predicate\n",
    "            prediction = dataset.predicates[preds[i].item()]  # Predicted predicate\n",
    "\n",
    "            # Image processing for visualization\n",
    "            image_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())  # Normalize to [0, 1]\n",
    "\n",
    "            # Generate CAM overlay\n",
    "            cam_image = show_cam_on_image(image_np, grayscale_cams[i], use_rgb=True)\n",
    "\n",
    "            # Display the image, CAM, and metadata\n",
    "            plt.imshow(cam_image)\n",
    "            plt.title(f\"Subject: {subject}, Object: {object_name}\\n\"\n",
    "                      f\"Ground Truth: {ground_truth}, Predicted: {prediction}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "            count += 1\n",
    "        if n_samples is not None and count >= n_samples:\n",
    "            break\n",
    "\n",
    "def compute_confusion_matrix(loader, model, dataset):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for the entire dataset based on model predictions.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader containing the dataset.\n",
    "        model: The ViT-based model for prediction.\n",
    "        dataset: The dataset instance to access predicates.\n",
    "        \n",
    "    Returns:\n",
    "        confusion matrix (2D array).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_idx, (images, labels, _, _) in enumerate(loader):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)  # Forward pass\n",
    "            preds = torch.argmax(outputs, dim=1)  # Predicted class\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(len(dataset.predicates)))\n",
    "    return cm\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Initialize data and model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "annotations_path = \"/kaggle/working/spatialsense_data/spatialsense/annots_spatialsenseplus.json\"\n",
    "extracted_images_path = \"/kaggle/working/spatialsense_data/spatialsense/images/images\"\n",
    "predicates = [\"above\", \"to the left of\", \"to the right of\", \"under\"]\n",
    "\n",
    "# Parse Annotations\n",
    "data = parse_annotations(annotations_path, extracted_images_path)\n",
    "\n",
    "# Train/Validation Split\n",
    "train_data = [item for item in data if item['split'] == 'train' and item['predicate'] in predicates]\n",
    "valid_data = [item for item in data if item['split'] == 'valid' and item['predicate'] in predicates]\n",
    "\n",
    "train_dataset = SpatialSenseDataset(train_data, predicates, transform=transform)\n",
    "valid_dataset = SpatialSenseDataset(valid_data, predicates, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Class Weights\n",
    "predicate_counts = Counter(item[\"predicate\"] for item in train_data)\n",
    "class_weights = [1.0 / predicate_counts[pred] if pred in predicate_counts else 1.0 for pred in predicates]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Initialize Models and Optimizer\n",
    "mlp_model = MLPModel(input_dim=768, num_classes=len(predicates)).to(device)\n",
    "full_model = FullModel(feature_extractor, mlp_model).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": feature_extractor.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": mlp_model.parameters(), \"lr\": 1e-4}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Train and evaluate the model\n",
    "history = train_and_evaluate(\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    model=full_model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    target_layer=full_model.feature_extractor.vit.encoder.layer[0].attention.output,  \n",
    "    predicates=predicates,\n",
    "    device=device,\n",
    "    num_epochs=100,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "# Plotting Functions\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"valid_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs. Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"valid_precision\"], label=\"Precision\")\n",
    "    plt.plot(history[\"valid_recall\"], label=\"Recall\")\n",
    "    plt.plot(history[\"valid_f1\"], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Validation Metrics: Precision, Recall, F1-Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Display Plots\n",
    "plot_loss(history)\n",
    "plot_metrics(history)\n",
    "\n",
    "# Generate confusion matrix for the entire validation set\n",
    "cm = compute_confusion_matrix(valid_loader, full_model, valid_dataset)\n",
    "\n",
    "# Convert confusion matrix to dataframe for easy visualization\n",
    "cm_df = pd.DataFrame(cm, index=valid_dataset.predicates, columns=valid_dataset.predicates)\n",
    "\n",
    "# Plot the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g', xticklabels=valid_dataset.predicates, yticklabels=valid_dataset.predicates)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "# Call the updated visualization function\n",
    "generate_eigencam_vit(valid_loader, full_model,valid_dataset, n_samples=5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
